import streamlit as st
import pandas as pd
from google.oauth2 import service_account
from googleapiclient.discovery import build
import io
import csv
import os

# ==========================================
# 1. ÙƒÙˆØ¯ Ø§Ù„Ø±ÙˆØ¨ÙˆØª (Ø§Ù„Ù…Ø­Ø±Ùƒ)
# ==========================================
FOLDER_ID = "1kgzKj9sn8pQVjr78XcN7_iF5KLmflwME"
SCOPES = ['https://www.googleapis.com/auth/drive.readonly']

COLUMN_MAPPING = {
    'Ø§Ù„Ø³Ø¹Ø±': 'Ø§Ù„Ø³Ø¹Ø±', 'Ù…Ø¨Ù„Øº Ø§Ù„ØµÙÙ‚Ø©': 'Ø§Ù„Ø³Ø¹Ø±', 'Price': 'Ø§Ù„Ø³Ø¹Ø±', 'Ù‚ÙŠÙ…Ø© Ø§Ù„ØµÙÙ‚Ø§Øª': 'Ø§Ù„Ø³Ø¹Ø±', 'Ø³Ø¹Ø± Ø§Ù„ÙˆØ­Ø¯Ø©': 'Ø§Ù„Ø³Ø¹Ø±',
    'Ø§Ù„Ù…Ø³Ø§Ø­Ø©': 'Ø§Ù„Ù…Ø³Ø§Ø­Ø©', 'Ø§Ù„Ù…Ø³Ø§Ø­Ø© Ø¨Ø§Ù„Ø£Ù…ØªØ§Ø±': 'Ø§Ù„Ù…Ø³Ø§Ø­Ø©', 'Area': 'Ø§Ù„Ù…Ø³Ø§Ø­Ø©', 'Ù…Ø³Ø§Ø­Ø© Ø§Ù„ÙˆØ­Ø¯Ø©': 'Ø§Ù„Ù…Ø³Ø§Ø­Ø©',
    'Ø§Ù„Ø­ÙŠ': 'Ø§Ù„Ø­ÙŠ', 'Ø§Ø³Ù… Ø§Ù„Ø­ÙŠ': 'Ø§Ù„Ø­ÙŠ', 'District Name': 'Ø§Ù„Ø­ÙŠ', 'Ø§Ù„Ù…ÙˆÙ‚Ø¹': 'Ø§Ù„Ø­ÙŠ',
    'Ù†ÙˆØ¹ Ø§Ù„Ø¹Ù‚Ø§Ø±': 'Ù†ÙˆØ¹_Ø§Ù„Ø¹Ù‚Ø§Ø±_Ø§Ù„Ø®Ø§Ù…', 'ØªØµÙ†ÙŠÙ Ø§Ù„Ø¹Ù‚Ø§Ø±': 'Ù†ÙˆØ¹_Ø§Ù„Ø¹Ù‚Ø§Ø±_Ø§Ù„Ø®Ø§Ù…', 'Ø§Ù„ÙˆØ­Ø¯Ø©': 'Ù†ÙˆØ¹_Ø§Ù„Ø¹Ù‚Ø§Ø±_Ø§Ù„Ø®Ø§Ù…', 'Ø§Ù„Ù†ÙˆØ¹': 'Ù†ÙˆØ¹_Ø§Ù„Ø¹Ù‚Ø§Ø±_Ø§Ù„Ø®Ø§Ù…',
    'Ø§Ù„Ù…Ø¯ÙŠÙ†Ø©': 'Ø§Ù„Ù…Ø¯ÙŠÙ†Ø©', 
    'Ø§Ù„Ù…Ø·ÙˆØ±': 'Ø§Ø³Ù…_Ø§Ù„Ù…Ø·ÙˆØ±', 'Ø§Ø³Ù… Ø§Ù„Ù…Ø´Ø±ÙˆØ¹': 'Ø§Ø³Ù…_Ø§Ù„Ù…Ø´Ø±ÙˆØ¹'
}

class RealEstateBot:
    def __init__(self):
        self.log_messages = []
        self.creds = self.get_creds()
        self.service = build('drive', 'v3', credentials=self.creds)
        self.df = self.load_data_from_drive()

    def log(self, msg):
        print(msg)
        self.log_messages.append(msg)

    def get_creds(self):
        # Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† Ø§Ù„Ø£Ø³Ø±Ø§Ø± ÙÙŠ Streamlit (Ù„Ù„Ù†Ø´Ø±)
        if 'gcp_service_account' in st.secrets:
            return service_account.Credentials.from_service_account_info(st.secrets['gcp_service_account'], scopes=SCOPES)
        # Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† Ø§Ù„Ù…Ù„Ù Ù…Ø­Ù„ÙŠØ§Ù‹ (Ù„Ù„ØªØ·ÙˆÙŠØ±)
        elif os.path.exists('credentials.json'):
            return service_account.Credentials.from_service_account_file('credentials.json', scopes=SCOPES)
        else:
            raise FileNotFoundError("âš ï¸ Ù„Ù… ÙŠØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ù…ÙØ§ØªÙŠØ­ Ø§Ù„Ø¯Ø®ÙˆÙ„ (Secrets Ø£Ùˆ credentials.json)")

    def load_data_from_drive(self):
        all_data = []
        self.log("ğŸ“‚ Ø¬Ø§Ø±ÙŠ Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† Ø§Ù„Ù…Ù„ÙØ§Øª...")
        
        try:
            results = self.service.files().list(
                q=f"'{FOLDER_ID}' in parents and trashed=false",
                fields="files(id, name)").execute()
            files = results.get('files', [])

            for file in files:
                if not file['name'].lower().endswith('.csv'):
                    continue
                
                try:
                    request = self.service.files().get_media(fileId=file['id'])
                    content_bytes = request.execute()
                    
                    try: content_str = content_bytes.decode('utf-8-sig')
                    except: content_str = content_bytes.decode('utf-16')

                    is_dev = any(x in file['name'].lower() for x in ['dev', 'Ù…Ø·ÙˆØ±', 'brochure', 'projects'])
                    
                    if is_dev:
                        df_temp = pd.read_csv(io.StringIO(content_str), sep=None, engine='python')
                        df_temp['Source_Type'] = 'Ø³ÙˆÙ‚_Ø­Ø§Ù„ÙŠ (Ù…Ø·ÙˆØ±ÙŠÙ†)'
                    elif 'MOJ' in file['name'].upper():
                        f = io.StringIO(content_str)
                        reader = csv.reader(f, delimiter=';')
                        header_row = None; data_rows = []
                        for row in reader:
                            clean_row = [str(cell).strip() for cell in row]
                            if 'Ø§Ù„Ø³Ø¹Ø±' in clean_row and 'Ø§Ù„Ø­ÙŠ' in clean_row: header_row = clean_row; continue
                            if header_row and len(clean_row) >= len(header_row): data_rows.append(clean_row[:len(header_row)])
                        if header_row: df_temp = pd.DataFrame(data_rows, columns=header_row)
                        else: continue
                        df_temp['Source_Type'] = 'ØµÙÙ‚Ø§Øª_Ù…Ù†ÙØ°Ø© (Ø§Ù„Ø¹Ø¯Ù„)'
                    else:
                        df_temp = pd.read_csv(io.StringIO(content_str), sep=None, engine='python')
                        df_temp['Source_Type'] = 'Ù…Ø¤Ø´Ø±Ø§Øª_Ø¹Ø§Ù…Ø©'

                    df_temp.columns = df_temp.columns.str.strip()
                    df_temp.rename(columns=COLUMN_MAPPING, inplace=True)
                    df_temp = df_temp.loc[:, ~df_temp.columns.duplicated()]

                    if 'Ø§Ù„Ù…Ø¯ÙŠÙ†Ø©' in df_temp.columns:
                        df_temp['Ø§Ù„Ù…Ø¯ÙŠÙ†Ø©'] = df_temp['Ø§Ù„Ù…Ø¯ÙŠÙ†Ø©'].astype(str).str.strip()
                        df_temp = df_temp[df_temp['Ø§Ù„Ù…Ø¯ÙŠÙ†Ø©'] == 'Ø§Ù„Ø±ÙŠØ§Ø¶']

                    for col in ['Ø§Ù„Ø³Ø¹Ø±', 'Ø§Ù„Ù…Ø³Ø§Ø­Ø©']:
                        if col in df_temp.columns:
                            df_temp[col] = df_temp[col].astype(str).str.replace(',', '').str.replace(r'[^\d.]', '', regex=True)
                            df_temp[col] = pd.to_numeric(df_temp[col], errors='coerce')

                    df_temp.dropna(subset=['Ø§Ù„Ø³Ø¹Ø±', 'Ø§Ù„Ù…Ø³Ø§Ø­Ø©'], inplace=True)
                    df_temp['Ø³Ø¹Ø±_Ø§Ù„Ù…ØªØ±'] = df_temp['Ø§Ù„Ø³Ø¹Ø±'] / df_temp['Ø§Ù„Ù…Ø³Ø§Ø­Ø©']
                    # Ø­ÙØ¸ Ø§Ø³Ù… Ø§Ù„Ù…Ù„Ù Ù„Ù„Ø¹Ø±Ø¶ ÙÙŠ Ø§Ù„Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª
                    df_temp['Source_File'] = file['name'] 
                    
                    if 'Ù†ÙˆØ¹_Ø§Ù„Ø¹Ù‚Ø§Ø±_Ø§Ù„Ø®Ø§Ù…' not in df_temp.columns: df_temp['Ù†ÙˆØ¹_Ø§Ù„Ø¹Ù‚Ø§Ø±_Ø§Ù„Ø®Ø§Ù…'] = "ØºÙŠØ± Ù…Ø­Ø¯Ø¯"
                    
                    cols = ['Ø§Ù„Ø­ÙŠ', 'Ø§Ù„Ø³Ø¹Ø±', 'Ø§Ù„Ù…Ø³Ø§Ø­Ø©', 'Ø³Ø¹Ø±_Ø§Ù„Ù…ØªØ±', 'Ù†ÙˆØ¹_Ø§Ù„Ø¹Ù‚Ø§Ø±_Ø§Ù„Ø®Ø§Ù…', 'Source_File', 'Source_Type', 'Ø§Ø³Ù…_Ø§Ù„Ù…Ø·ÙˆØ±']
                    all_data.append(df_temp[[c for c in cols if c in df_temp.columns]])

                except Exception as e:
                    self.log(f"Ø®Ø·Ø£ ÙÙŠ Ù…Ù„Ù {file['name']}: {e}")

            if all_data:
                total_df = pd.concat(all_data, ignore_index=True)
                medians = total_df.groupby('Ø§Ù„Ø­ÙŠ')['Ø³Ø¹Ø±_Ø§Ù„Ù…ØªØ±'].median().to_dict()

                def classify(row):
                    raw = str(row.get('Ù†ÙˆØ¹_Ø§Ù„Ø¹Ù‚Ø§Ø±_Ø§Ù„Ø®Ø§Ù…', '')).strip().lower()
                    if row.get('Source_Type') == 'Ø³ÙˆÙ‚_Ø­Ø§Ù„ÙŠ (Ù…Ø·ÙˆØ±ÙŠÙ†)':
                        if 'Ø´Ù‚Ø©' in raw: return 'Ù…Ø¨Ù†ÙŠ (Ø´Ù‚Ø© - Ù…Ø·ÙˆØ±)'
                        if 'ÙÙŠÙ„Ø§' in raw: return 'Ù…Ø¨Ù†ÙŠ (ÙÙŠÙ„Ø§ - Ù…Ø·ÙˆØ±)'
                        if 'Ø£Ø±Ø¶' in raw: return 'Ø£Ø±Ø¶ (Ù…Ø·ÙˆØ±)'
                    if 'ØªØ¬Ø§Ø±ÙŠ' in raw: return "Ø£Ø±Ø¶ (ØªØ¬Ø§Ø±ÙŠ)"
                    area, ppm, dist = row['Ø§Ù„Ù…Ø³Ø§Ø­Ø©'], row['Ø³Ø¹Ø±_Ø§Ù„Ù…ØªØ±'], row['Ø§Ù„Ø­ÙŠ']
                    if area < 200: return "Ù…Ø¨Ù†ÙŠ (Ø´Ù‚Ø©)"
                    avg = medians.get(dist, 0)
                    if avg > 0 and ppm > (avg * 1.5) and area < 900: return "Ù…Ø¨Ù†ÙŠ (ÙÙŠÙ„Ø§/Ø¨ÙŠØª)"
                    return "Ø£Ø±Ø¶"

                total_df['Ù†ÙˆØ¹_Ø§Ù„Ø¹Ù‚Ø§Ø±'] = total_df.apply(classify, axis=1)
                return total_df
            return pd.DataFrame()
        except Exception as e:
            self.log(f"Ø®Ø·Ø£ Ø¹Ø§Ù…: {e}")
            return pd.DataFrame()

# ==========================================
# 2. ÙˆØ§Ø¬Ù‡Ø© Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù… (Dashboard UI)
# ==========================================
st.set_page_config(page_title="Ø§Ù„Ù…Ø­Ù„Ù„ Ø§Ù„Ø¹Ù‚Ø§Ø±ÙŠ Ø§Ù„Ø°ÙƒÙŠ", layout="wide", page_icon="ğŸ¢")

# ---------------- Ø§Ù„Ù‚Ø§Ø¦Ù…Ø© Ø§Ù„Ø¬Ø§Ù†Ø¨ÙŠØ© (Ù…Ø¹ Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø§Ù„Ù…Ù„ÙØ§Øª) ----------------
with st.sidebar:
    st.header("âš™ï¸ Ø§Ù„ØªØ­ÙƒÙ…")
    if st.button("ğŸ”„ ØªØ­Ø¯ÙŠØ« Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª", use_container_width=True, type="primary"):
        st.cache_data.clear()
        st.rerun()
    
    # Ù‡Ù†Ø§ Ø§Ù„Ø¥Ø¶Ø§ÙØ© Ø§Ù„Ø¬Ø¯ÙŠØ¯Ø©: Ø¥Ø¸Ù‡Ø§Ø± Ù…ØµØ§Ø¯Ø± Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
    if 'bot' in st.session_state and hasattr(st.session_state.bot, 'df'):
        df_stats = st.session_state.bot.df
        if not df_stats.empty:
            st.divider()
            st.markdown("### ğŸ“ Ù…ØµØ§Ø¯Ø± Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª")
            st.markdown("Ø¹Ø¯Ø¯ Ø§Ù„ØµÙÙ‚Ø§Øª ÙÙŠ ÙƒÙ„ Ù…Ù„Ù:")
            
            # Ø­Ø³Ø§Ø¨ Ø¹Ø¯Ø¯ Ø§Ù„ØµÙÙ‚Ø§Øª Ù„ÙƒÙ„ Ù…Ù„Ù
            file_counts = df_stats['Source_File'].value_counts().reset_index()
            file_counts.columns = ['Ø§Ø³Ù… Ø§Ù„Ù…Ù„Ù', 'Ø¹Ø¯Ø¯ Ø§Ù„ØµÙÙ‚Ø§Øª']
            
            # Ø¹Ø±Ø¶Ù‡Ø§ ÙƒØ¬Ø¯ÙˆÙ„ ØµØºÙŠØ±
            st.dataframe(file_counts, hide_index=True, use_container_width=True)

# ---------------- Ø§Ù„Ø´Ø§Ø´Ø© Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ© ----------------
st.title("ğŸ§ Ù…Ø¯Ù‚Ù‚ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø¹Ù‚Ø§Ø±ÙŠØ© (Ø§Ù„Ù†Ø³Ø®Ø© Ø§Ù„Ù…ÙˆØ­Ø¯Ø©)")

# ØªØ´ØºÙŠÙ„ Ø§Ù„Ø±ÙˆØ¨ÙˆØª
if 'bot' not in st.session_state:
    with st.spinner("Ø¬Ø§Ø±ÙŠ Ø§Ù„Ø§ØªØµØ§Ù„ Ø¨Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª..."):
        try:
            st.session_state.bot = RealEstateBot()
        except Exception as e:
            st.error(f"Ø­Ø¯Ø« Ø®Ø·Ø£: {e}")

if 'bot' in st.session_state and hasattr(st.session_state.bot, 'df'):
    df = st.session_state.bot.df
    
    if df.empty:
        st.warning("âš ï¸ Ù„Ø§ ØªÙˆØ¬Ø¯ Ø¨ÙŠØ§Ù†Ø§Øª. ØªØ£ÙƒØ¯ Ù…Ù† ØµØ­Ø© Ø§Ù„Ù…ÙØ§ØªÙŠØ­ ÙˆØ§Ù„Ù…Ù„ÙØ§Øª.")
    else:
        # Ø§Ù„ÙÙ„ØªØ±Ø©
        st.markdown("### ğŸ§¹ ÙÙ„ØªØ±Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª")
        c1, c2 = st.columns(2)
        with c1: min_p = st.number_input("Ø£Ù‚Ù„ Ø³Ø¹Ø± Ù…ØªØ±:", value=500, step=100)
        with c2: max_p = st.number_input("Ø£Ø¹Ù„Ù‰ Ø³Ø¹Ø± Ù…ØªØ±:", value=25000, step=1000)

        clean_df = df[(df['Ø³Ø¹Ø±_Ø§Ù„Ù…ØªØ±'] >= min_p) & (df['Ø³Ø¹Ø±_Ø§Ù„Ù…ØªØ±'] <= max_p)].copy()
        
        st.divider()
        st.markdown("### ğŸ” Ø§Ù„Ø¨Ø­Ø« ÙˆØ§Ù„ØªØ­Ù„ÙŠÙ„")
        
        sc1, sc2 = st.columns([3, 1])
        search = sc1.text_input("Ø§Ø³Ù… Ø§Ù„Ø­ÙŠ:", "Ø§Ù„Ù…Ù„Ù‚Ø§")
        
        if sc2.button("Ø¹Ø±Ø¶ ğŸ“Š", use_container_width=True, type="primary") or search:
            res = clean_df[clean_df['Ø§Ù„Ø­ÙŠ'].astype(str).str.contains(search, na=False)]
            
            if res.empty:
                st.info(f"Ù„Ø§ ØªÙˆØ¬Ø¯ Ù†ØªØ§Ø¦Ø¬ Ù„Ø­ÙŠ '{search}'")
            else:
                l_df = res[res['Ù†ÙˆØ¹_Ø§Ù„Ø¹Ù‚Ø§Ø±'].str.contains('Ø£Ø±Ø¶', na=False)]
                b_df = res[res['Ù†ÙˆØ¹_Ø§Ù„Ø¹Ù‚Ø§Ø±'].str.contains('Ù…Ø¨Ù†ÙŠ', na=False)]
                
                m1, m2, m3, m4 = st.columns(4)
                m1.metric("Ø£Ø±Ø§Ø¶ÙŠ", f"{len(l_df):,}")
                m2.metric("Ù…ØªÙˆØ³Ø· Ø£Ø±Ø¶", f"{l_df['Ø³Ø¹Ø±_Ø§Ù„Ù…ØªØ±'].median():,.0f}")
                m3.metric("Ù…Ø¨Ø§Ù†ÙŠ", f"{len(b_df):,}")
                m4.metric("Ù…ØªÙˆØ³Ø· Ù…Ø¨Ù†Ù‰", f"{b_df['Ø³Ø¹Ø±_Ø§Ù„Ù…ØªØ±'].median():,.0f}")
                
                # Ø¹Ø±Ø¶ Ø§Ù„Ù…ØµØ¯Ø± (Ø§Ø³Ù… Ø§Ù„Ù…Ù„Ù) ÙÙŠ Ø§Ù„Ø¬Ø¯ÙˆÙ„ Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠ Ø£ÙŠØ¶Ø§Ù‹
                st.dataframe(res[['Ø§Ù„Ø­ÙŠ', 'Ù†ÙˆØ¹_Ø§Ù„Ø¹Ù‚Ø§Ø±', 'Ø§Ù„Ù…Ø³Ø§Ø­Ø©', 'Ø§Ù„Ø³Ø¹Ø±', 'Ø³Ø¹Ø±_Ø§Ù„Ù…ØªØ±', 'Source_File']].style.format({'Ø§Ù„Ø³Ø¹Ø±':'{:,.0f}', 'Ø³Ø¹Ø±_Ø§Ù„Ù…ØªØ±':'{:,.0f}'}), use_container_width=True)
